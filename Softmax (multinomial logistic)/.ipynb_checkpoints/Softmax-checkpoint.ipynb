{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax\n",
    "-------------------------\n",
    "## notMNIST dataset \n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, in addition when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "## Cross-entropy\n",
    "We will be using the loss of our model with the following function\n",
    "\n",
    "$$ H_{y'}(y) = -\\sum_i y'_i \\log(y_i) $$\n",
    "\n",
    "Where is our predicted probability distribution, and is the true distribution (the one-hot vector with the digit labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize the dataset...\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Datasets/notMNIST_small/A/Q2hvY0lDRy5vdGY=.png\n"
     ]
    }
   ],
   "source": [
    "path = \"./Datasets/notMNIST_small/{}/{}\"\n",
    "\n",
    "A = np.array(os.listdir(\"./Datasets/notMNIST_small/A/\"))\n",
    "B = np.array(os.listdir(\"./Datasets/notMNIST_small/B/\"))\n",
    "C = np.array(os.listdir(\"./Datasets/notMNIST_small/C/\"))\n",
    "D = np.array(os.listdir(\"./Datasets/notMNIST_small/D/\"))\n",
    "E = np.array(os.listdir(\"./Datasets/notMNIST_small/E/\"))\n",
    "F = np.array(os.listdir(\"./Datasets/notMNIST_small/F/\"))\n",
    "G = np.array(os.listdir(\"./Datasets/notMNIST_small/G/\"))\n",
    "H = np.array(os.listdir(\"./Datasets/notMNIST_small/H/\"))\n",
    "I = np.array(os.listdir(\"./Datasets/notMNIST_small/I/\"))\n",
    "J = np.array(os.listdir(\"./Datasets/notMNIST_small/J/\"))\n",
    "\n",
    "files = np.stack((A, B, C, D, E, F, G, H, I, J), axis=0)\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\",\"F\", \"G\", \"H\", \"I\", \"J\"]\n",
    "\n",
    "## The above ar np arrays holding the names of the files that contains each of the letters in notMNIST\n",
    "print (path.format(\"A\", A[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB6UlEQVR4nFWSPWgUURSFz71vtnDF\nJdlVEpN1gygxoKSQoKhYChF/ooKFhYV/nQEtbGxsLOwsgqCdoIUIoqYSg0rSKKKFdSIoEhDXJYbA\nLLvz3j0W85PNK+eb85jzzRH0Hq3unxgbrPS/nu9/0d5A4LT0joEkGfgIsgEJtn5i14cQgudqvYeK\nYnRmJQvS2JmAFgzjz2JaxmgMR+AKtv0XQ2LdZgEPwWVRxe16R0Se34OlT2hFBexZpRlbteP0adIf\ngGp267WKF+JW62/+GUVSMNSkeT4BGi0aSWN7b/aaw00mgcuDQN8SA0nj2s78Dv3G4HkVEdwXepLG\nlYFUgsMJBs85LTmHDxn8sxkaiYOGy7AovmEwzZwJ+t5ueRrRq983CcHX0dNDO4ar9UxM6aBTaUyO\nVA+PU2CFyqxKcgwzaWuSPkl8yOUy4VIZjUV2AkmzEIL33pj90O/nAVygkWbccAIfOoiozZ4KjpLE\ncdxur/07uo2C4F6eEw2IcIldfhzbPVwrKxTv6UnjSUQAFI/Z5dm0gJQwR0/P5UpauPKbXFB1KiJw\neMOu95zNZEzR80w+CcECEzbvj2TwATkvhbXoM39M1/LhbVrkelAx8JMX4fL1TDG+m88Mil2vrpSl\nEHknvo5oXalIz5b/A0UeQmvcumJgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F413857E828>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take a look at the images\n",
    "from PIL import Image\n",
    "im = Image.open(path.format(\"A\", A[5]))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPixels(im):\n",
    "    pixels = list(im.getdata())\n",
    "    width, height = im.size\n",
    "    return np.array([pixels[i * width:(i + 1) * width] for i in range(height)]).reshape(width * height)\n",
    "\n",
    "getPixels(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   2, ..., 252, 255, 181],\n",
       "       [  0,   1,   1, ..., 255, 255, 147],\n",
       "       [  0,   0,   0, ..., 255, 255, 255],\n",
       "       ..., \n",
       "       [  0,   0,   0, ..., 246, 252, 206],\n",
       "       [  0,   0,   0, ...,   0,   3,   1],\n",
       "       [  0,   0,   0, ..., 253, 255, 108]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieveImages (array, letter):\n",
    "    return np.array([getPixels(Image.open(path.format(letter, fname))) for fname in array])\n",
    "\n",
    "retrieveImages(A, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1873, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, all is ready to make our trainig matrix!\n",
    "data = np.stack((retrieveImages(files[i], labels[i]) for i in range(10)), axis=0)\n",
    "## 10 classes | 1873 examples of each | 784 pixels/dimensions\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.diag(np.ones(10))\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're now ready to actually make our model!\n",
    "------------------\n",
    "A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.\n",
    "\n",
    "To tally up the evidence that a given image is in a particular class, we do a weighted sum of the pixel intensities. The weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor.\n",
    "\n",
    "$$ \\text{evidence}_i = \\sum_j W_{i,~ j} x_j + b_i $$\n",
    "\n",
    "We then convert the evidence tallies into our predicted probabilities using the \"softmax\" function:\n",
    "\n",
    "$$ y_i = \\text{softmax}(\\text{evidence}_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feels good\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# x isn't a specific value. It's a placeholder, a value that we'll input when we ask TensorFlow to run a computation. \n",
    "# We want to be able to input any number of images, each flattened into a 784-dimensional vector\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# Variable. A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. \n",
    "# It can be used and even modified by the computation. \n",
    "# For machine learning applications, one generally has the model parameters be Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement our model\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "---------------\n",
    "Recall our loss function will be cross-entropy:\n",
    "$$ H_{y'}(y) = -\\sum_i y'_i \\log(y_i) $$\n",
    "Where y is our predicted probability distribution, and y' is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder to input the correct answers\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# define loss function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# train using the gradient descent algorithm with a learning rate of 0.5.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_xs, batch_ys = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBatch (i, n):\n",
    "    # i is the letter from which we want the random sample\n",
    "    # n is the number of samples\n",
    "    \n",
    "    # will return n random examples of letters in notMNST and its labels\n",
    "    xs = data[i][np.random.randint(data[i].shape[0], size=n), :]\n",
    "    ys = np.repeat([classes[i]], n, axis = 0)\n",
    "    return xs, ys\n",
    "\n",
    "def getGeneralBatch (n):\n",
    "    #will return a batch with batches of n/10 the size for every class\n",
    "    #long story short, a random batch containing 1/10 intances of every class\n",
    "    nc = 10\n",
    "    \n",
    "    xs, ys = getBatch(0, int(n/nc))\n",
    "    for i in range(1, nc):\n",
    "        newXs, newYs = getBatch(i, int(n/nc))\n",
    "        xs = np.concatenate((xs, newXs))\n",
    "        ys = np.concatenate((ys, newYs))\n",
    "        \n",
    "    return xs, ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs, ys = getGeneralBatch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 103, 108,  73],\n",
       "       [  0,   0,   0, ...,  78, 101,  21],\n",
       "       [255, 255, 255, ...,   2,   0,   0],\n",
       "       ..., \n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
