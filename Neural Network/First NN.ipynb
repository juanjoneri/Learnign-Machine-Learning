{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multui layer Neural Network:\n",
    "This is a very early approach to make a 2 and 3 leyer neural network that can easily be traced and understood\n",
    "This network therefore computes very simple predictions in a very small dataset described by [this](http://iamtrask.github.io/2015/07/12/basic-python-network/) tutorial by iamtrask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Neural Net\n",
    "\n",
    "|Input  |Output|\n",
    "|-------|------|\n",
    "|[0,0,1]|0     |\n",
    "|[1,1,1]|1     |\n",
    "|[1,0,1]|1     |\n",
    "|[0,1,1]|0     |\n",
    "\n",
    "We can see that the leftmost input column is perfectly correlated with the output.\n",
    "We therefore expect Backpropagation to measure statistics like this to make train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966968]\n",
      " [ 0.00786504]\n",
      " [ 0.99358967]\n",
      " [ 0.99211616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "# maps any value to a value between 0 and 1.\n",
    "# use it to convert numbers to probabilities.\n",
    "def sigmoid (x, deriv = False):\n",
    "    # implement the gradient inside for convenience\n",
    "    if(deriv):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Data \n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "          \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# synapse zero:  weight matrix\n",
    "# - Only one for one layer\n",
    "# - Its dimension is (3,1) because we have 3 inputs and 1 output.\n",
    "syn0 = 2 * np.random.random((3,1)) - 1\n",
    "\n",
    "for _ in range(10000):\n",
    "\n",
    "    # make a prediction\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0, syn0))\n",
    "    # this returns a 4 by 1 vector with the expected values of our inputs\n",
    "\n",
    "    # calculate a simple linear error\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply the error by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    # Because sigmoid is higher in 0 regien when we have low confidence\n",
    "    # and we want to change those weight more heavily\n",
    "    l1_delta = l1_error * sigmoid(l1, True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T, l1_delta)\n",
    "\n",
    "print (\"Output After Training:\\n{}\".format(l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable|Definition|\n",
    "|--------|----------|\n",
    "|X|Input dataset matrix where each row is a training example|\n",
    "|y|Output dataset matrix where each row is a training example|\n",
    "|l0|First Layer of the Network, specified by the input data|\n",
    "|l1|Second Layer of the Network, otherwise known as the hidden layer|\n",
    "|syn0|First layer of weights, Synapse 0, connecting l0 to l1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer Neural Net:\n",
    "Our first layer will combine the inputs, and our second layer will then map them to the output using the output of the first layer as input. \n",
    "\n",
    "|Input  |Output|\n",
    "|-------|------|\n",
    "|[0,0,1]|0     |\n",
    "|[1,1,1]|1     |\n",
    "|[1,0,1]|1     |\n",
    "|[0,1,1]|0     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.500000030718\n",
      "Error: 0.0100930169264\n",
      "Error: 0.0069060210652\n",
      "Error: 0.00555848815644\n",
      "Error: 0.00477250290804\n",
      "Error: 0.00424329637213\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00411882]\n",
      " [ 0.99619376]\n",
      " [ 0.99617695]\n",
      " [ 0.00367733]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x, deriv = False):\n",
    "    if(deriv == True):\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2 * np.random.random((3,4)) - 1\n",
    "syn1 = 2 * np.random.random((4,1)) - 1\n",
    "\n",
    "for j in range(60000):\n",
    "\n",
    "    # make a prediction\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # calculate the error\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    # print the error every 10_000 steps to see if working\n",
    "    if (j % 10000) == 0:\n",
    "        print (\"Error: {}\".format(str(np.mean(np.abs(l2_error)))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error * nonlin(l2, deriv = True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    l1_delta = l1_error * nonlin(l1, deriv = True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "\n",
    "print (\"\\nOutput After Training:\\n{}\".format(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "|Variable|Definition|\n",
    "|--------|----------|\n",
    "|X|Input dataset matrix where each row is a training example|\n",
    "|y|Output dataset matrix where each row is a training example|\n",
    "|l0|First Layer of the Network, specified by the input data|\n",
    "|l1|Second Layer of the Network, otherwise known as the hidden layer|\n",
    "|l1|Final Layer of the Network, which is our hypothesis, and should approximate the correct answer as we train.|\n",
    "|syn0|First layer of weights, Synapse 0, connecting l0 to l1|\n",
    "|syn1|Second layer of weights, Synapse 1 connecting l1 to l2.|\n",
    "|l2_error|This is the amount that the neural network \"missed\".|\n",
    "|l2_delta|This is the error of the network scaled by the confidence. It's almost identical to the error except that very confident errors are muted.|\n",
    "|l1_error|Weighting l2_delta by the weights in syn1, we can calculate the error in the middle/hidden layer.|\n",
    "|l1_delta|This is the l1 error of the network scaled by the confidence. Again, it's almost identical to the l1_error except that confident errors are muted.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
